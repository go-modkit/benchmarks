# Benchmark Publication and Reproducibility Policy

This document defines the minimum disclosure requirements and operational bounds for publishable benchmark results generated by this harness. Adherence to this policy ensures that published claims are reproducible, statistically sound, and fair.

## Minimum Disclosure Requirements

Any public claim or report referencing benchmarks from this repository must include or link to the following artifacts from the `results/latest/` directory:

### 1. Environment Manifest and Fingerprint
- **Artifacts**: `environment.manifest.json`, `environment.fingerprint.json`
- **Requirement**: Disclosure of the runner metadata, toolchain versions (Go, Python, Docker), and host environment characteristics.
- **Purpose**: Ensures results can be contextualized against the hardware and software stack used during the run.

### 2. Schema Validation
- **Requirement**: All artifacts must pass `make benchmark-schema-validate`.
- **Purpose**: Guarantees that the raw metrics and summaries adhere to the versioned data contract.

### 3. Quality Policy Compliance
- **Requirement**: The run must pass `make ci-benchmark-quality-check` as defined in `stats-policy.json`.
- **Metrics**: Must include at least RPS (median) and Latency (P50, P95, P99).
- **Variance**: Coefficient of Variation (CV) must stay within the thresholds defined in `stats-policy.json` (e.g., <10% for RPS).

### 4. Parity and Skip Transparency
- **Requirement**: Any target implementation that failed the parity gate must be disclosed as "Skipped (Parity Failure)".
- **Purpose**: Prevents "cherry-picking" results from implementations that do not yet meet the functional contract.

## Operational Bounds

To maintain comparability and prevent resource exhaustion in CI/CD environments, publishable runs must stay within the following bounds (aligned with `benchmark-manual` workflow):

| Parameter | Range | Default |
|-----------|-------|---------|
| **Runs per target** | 1 .. 10 | 3 |
| **Requests per run** | 50 .. 1000 | 300 |
| **Frameworks** | Subset of `modkit,nestjs,baseline,wire,fx,do` | - |

Runs exceeding these bounds are considered "Experimental" and should not be used for official framework comparisons or public performance claims.

## Reproducibility Workflow

To generate a publishable report, follow the standard workflow:

1. **Clean Environment**: Ensure a clean working tree and stable runtime versions.
2. **Execution**:
   ```bash
   make benchmark
   ```
3. **Validation**:
   ```bash
   make benchmark-schema-validate
   make ci-benchmark-quality-check
   ```
4. **Reporting**:
   ```bash
   make report
   ```

The resulting `results/latest/report.md` and `results/latest/summary.json` are the authoritative sources for publication.

## Fairness and Interpretation

- **Fairness**: All runs must adhere to the principles in [METHODOLOGY.md](../../METHODOLOGY.md).
- **Disclaimer**: Every published report must include the fairness disclaimer regarding cross-language comparisons and runtime effects.
- **Correctness First**: Performance data from a run with parity failures is invalid for comparative claims.

## Policy Enforcement

The `make publication-sync-check` command (and its CI equivalent) verifies that published artifacts align with the current methodology and versioned policy.
